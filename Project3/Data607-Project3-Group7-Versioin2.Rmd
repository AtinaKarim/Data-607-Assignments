---
title: "Most Valued Data Science Skills"
author: "Group 7"
date: "March 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---------------

### Project 3 

> W. Edwards Deming said, "In God we trust, all others must bring data." Please use data to answer the question, "Which are the most valued data science skills?" Consider your work as an exploration; there is not necessarily a "right answer."

---------------

### Active group members

* AHM Shahparan
* Alejandro Osborne 
* Brian Liles
* Sherranette Tinapunan

---------------

### Load libraries

```{r warning=FALSE, message=FALSE}
library(rvest)
library(knitr)
library(tm)
library(stringr)
```

---------------

## Web scraping

### Load job posting URLs

The job URL file contains 100 entries. It has three columns: row_id, job_url, and selector. 
The `selector` column store the selector item as determined by `selectorgadget` tool.

```{r}
#setwd("C:/Users/stina/Documents/GitHub/Data-607-Assignments/Project3")

url_source <- 
  "https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/100Jobs-Shahparan-Version2.csv"

#load the list of URLs
url_listing <- data.frame(read.csv(url_source, header=TRUE, sep=",", stringsAsFactors = FALSE, row.names = NULL))
```

### Preview of URL list
```{r echo=FALSE}
row.names(url_listing) <- NULL
kable(url_listing[12:18,], format="markdown")
```

---------------

### Extract and save the job posting

While manually stepping through each URL to confirm that the selector grabs data from each page, I noticed that some of the URLs started to expire. In addition, there was a post in the class slack channel about being banned from a job site. So while going through the the step by step check, a line of code saved each extracted data to a text file just in case the posting expires or access gets denied. However, when an attempt was made to run the entire for loop to go through the 100 URLs to automatically build the dataset of job postings, Monster.com had already blocked access to the site. 

```{r echo=FALSE, out.width='80%'}
knitr::include_graphics("./images/Monster.com-Forbidden Access-3.23.2018-snippet.png")
```

The code below will go through each URL, scrape the web page for the data sepcified by the selector, and save the output as a text file. This code will generate 100 text files in the specified directory. The code has been commented out for the reason mentioned above.
```{r}
setwd("./JobpostingText")

#for (i in 1:nrow(url_listing)){
#  read_html(url_listing$job_url[i]) %>% 
#      html_nodes(url_listing$selector[i]) %>% html_text() -> data
  
#  filename <- paste("job", i, ".txt")
#  fileConn<-file(filename)
#  writeLines(c(data), fileConn)
#  close(fileConn)
#}
```


## Text mining and data clean up

The `tm` library is used to clean up the data and build a term document matrix. 
The code below will grab all 100 job postings under the specified directory. 
```{r}
cname <- "./Jobposting-Archive"
docs <- VCorpus(DirSource(cname))   
#summary(docs) 
```

### Preview first document

```{r echo=FALSE}
#writeLines(as.character(docs[1]))
#writeLines(as.character(docs))
inspect(docs[[1]])
```


### Data clean up

The clean up steps below, which is done by a mapper function called `tm_map` of the `tm` library is used to 

* remove punctuations
* remove numbers
* remove unecessary whitespaces
* convert to lowercase
* substitute multi-term keywrods into a string with no whitespaces (list of terms from a file)
* remove stop words
* remove irrelevant words (list of terms from a file)

For the multi-term keyword substitution, a team member manually built a list of these words. 

For the removal of irrelevant words, a pre-processing of the term document matrix was done first so we can investigate the words that are present in the dataset. The term document matrix was adjusted so that if a term is present in a document, it is only counted once. Mentioning the term more than once in a single job posting does not increase its relevance. A term frequency list is generated and reviewed by a team member. Building a list of irrelevant words will help lower down the presence of noise words if we do decide to run a different set of job postings to analyze. 

About preserving the term "r" in the dataset. 

### Convert to lowercase
```{r}
docs <- tm_map(docs, tolower)   
docs <- tm_map(docs, PlainTextDocument)
inspect(docs[[1]])
```

```{r}
clean.text <- function(text){
  str_replace_all(text, regex('\r\n|\n|\t|\r|,|/|<|>|\\.'), ' ')}
docs <- tm::tm_map(docs, clean.text)
docs <- tm_map(docs, PlainTextDocument)
inspect(docs[[1]])
```


### Remove punctuation
```{r}
for (j in seq(docs)) {

    docs[[j]]$content <- gsub("[[:punct:][:blank:]]+", " ", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
inspect(docs[[1]])
```

### Identify characters that are NOT letters
```{r}
content <- ""
for(i in 1:length(docs)){
 
  content <- paste(content, paste(as.character(docs[[i]]), collapse = " "))
  
}
notLetters <- unlist(str_extract_all(content, '[^A-Za-z ]'))
notLetters <- unique(unlist(strsplit(notLetters, "[ ]")))
```

### Special characters identified
```{r echo=FALSE}
kable(notLetters, format="markdown")
```

### Remove the special characters
```{r}
for (j in seq(docs)) {
  for (i in 1:length(notLetters)){
    docs[[j]]$content <- gsub(notLetters[i], " ", docs[[j]])
  }
}
inspect(docs[[1]])
```

### Remove Whitespace
```{r}
docs <- tm_map(docs, stripWhitespace)
inspect(docs[[1]])
```

```{r}
docs[[1]] <- gsub("r[ ]{1,}|[ ]{1,}r[ ]{1,}|[ ]{1,}r", " r-programming ", docs[[1]], ignore.case = TRUE)
docs <- tm_map(docs, PlainTextDocument)
inspect(docs[[1]])
```





### Combining words that should stay together

This takes a list of multi-term data science keywords from a file. This file was prepared by a team member.
These keywords will be preserved together by concatenating each term with a symbol to eliminate the whitespace in between the terms. 

#### Preview list of multi-term keywords to preserve
```{r}
file <-
  "https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/Multi-part%20DS%20terms_Shahparan-Version2.csv"

terms <- 
  read.csv(file, header=TRUE, sep=",", stringsAsFactors = FALSE, row.names = NULL)

kable(head(terms,10), format="markdown")
```


#### Replace the multi-term keywords
```{r}
for (j in seq(docs))
{
  #docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
  for(i in 1:nrow(terms)){
    docs[[j]] <- gsub(terms$Terms[i], terms$Replace[i], docs[[j]], ignore.case = TRUE)
  }
}
docs <- tm_map(docs, PlainTextDocument)

inspect(docs[[1]])
```


### Remove stop words
```{r}
# For a list of the stopwords, see:   
# length(stopwords("english"))   
# stopwords("english")   
docs <- tm_map(docs, removeWords, stopwords("english"))   
docs <- tm_map(docs, PlainTextDocument)
```

### Removing irrelevant words

The initial term frequency output was analyzed by a team member. 
```{r}
file <-
  "https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/RemoveTerms-Score20andHigher.csv"

terms <- 
  read.csv(file, header=TRUE, sep=",", stringsAsFactors = FALSE, row.names = NULL)

terms <- unlist(terms)

docs <- tm_map(docs, removeWords, terms)   
```

### Remove whitespace
```{r}
docs <- tm_map(docs, stripWhitespace)
inspect(docs[[1]])
```

## Stage the Data

### create a term document matrix.

There are 100 documents with 5,431 terms.
```{r}
tdm <- TermDocumentMatrix(docs) 
tdm  
```


## Explore your data

### Preview
```{r}
#m <- as.matrix(tdms)
m <- as.matrix(tdm)

for(i in 1:nrow(m)){
  
  for(j in 1:ncol(m)){
    if(m[i,j]>1){
      m[i,j] <- 1
    }
  }
}

freq <- rowSums(m)
termFreq <- cbind(names(freq),freq)
rownames(termFreq) <- NULL
colnames(termFreq) <- c("term", "frequency")
termFreq <- data.frame(termFreq)

#write.csv(m, file="TermDocumentMatrix_adjusted.csv") 
write.csv(termFreq, file="TermFrequency_adjusted.csv") 
```

### Export to Excel for further investigation
```{r}
m <- as.matrix(tdms)
freq <- rowSums(m)
names(freq)
dim(m)   
#write.csv(m, file="DocumentTermMatrix2.csv")  
write.csv(m, file="TermDocumentMatrix.csv")  
```



