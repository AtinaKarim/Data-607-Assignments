---
title: "Most Valued Data Science Skills"
author: "Group 7"
date: "March 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---------------

### Project 3 

> W. Edwards Deming said, "In God we trust, all others must bring data." Please use data to answer the question, "Which are the most valued data science skills?" Consider your work as an exploration; there is not necessarily a "right answer."

---------------

### Active group members

* AHM Shahparan
* Alejandro Osborne 
* Brian Liles
* Sherranette Tinapunan

---------------

### Load libraries

```{r warning=FALSE, message=FALSE}
library(rvest)
library(knitr)
library(tm)
library(stringr)
```

---------------

## Web scraping

### Load job posting URLs

The job URL file contains 100 entries. It has three columns: row_id, job_url, and selector. 
The `selector` column store the selector item as determined by `selectorgadget` tool.

```{r}
#setwd("C:/Users/stina/Documents/GitHub/Data-607-Assignments/Project3")

url_source <- 
  "https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/100Jobs-Shahparan-Version2.csv"

#load the list of URLs
url_listing <- read.csv(url_source, header=TRUE, sep=",") 

url_listing$job_url <- as.character(url_listing$job_url)
url_listing$selector <- as.character(url_listing$selector)
```

### Preview of URL list
```{r}
rownames(url_listing) <- NULL
kable(url_listing[12:18,], format="markdown")
```

---------------

### Extract and save the job posting

While manually stepping through each URL to confirm that the selector grabs text from each page, I noticed that some of the URLs started to expire. In addition, there was a post in the class slack channel about being banned from a job site. So while going through the the step by step check, a line of code saved each extracted data to a text file. However, when an attempt was made to run the entire for loop to go through the 100 URLs to automatically build the dataset of job postings, Monster.com had already blocked access to the site. 

```{r echo=FALSE, out.width='70%'}
knitr::include_graphics("./images/Monster.com-Forbidden Access-3.23.2018-snippet.png")
```

The code below will go through each URL, scrape the web page for the data sepcified by the selector, and save the output as a text file. This code will generate 100 text files.  

The code has been commented out for the reason mentioned above.
```{r}
setwd("./JobpostingText")

#for (i in 1:nrow(url_listing)){
#  read_html(url_listing$job_url[i]) %>% 
#      html_nodes(url_listing$selector[i]) %>% html_text() -> data
  
#  filename <- paste("job", i, ".txt")
#  fileConn<-file(filename)
#  writeLines(c(data), fileConn)
#  close(fileConn)
#}
```


## Text mining and data clean up

The `tm` library is used to clean up the data and build a term document matrix. 

The code below will grab all 100 job postings under the specified directory. 

```{r}
cname <- "../Jobposting-Archive"
docs <- VCorpus(DirSource(cname))   
#summary(docs) 
```

```{r }
typeof(docs) #list
inspect(docs[1])
meta(docs[[1]], "id") #get the ID
meta(docs[[1]]) #get meta data
inspect(docs[[1]]) #you see the actual document content
writeLines(as.character(docs)) #loads all the text in all the files
writeLines(as.character(docs[1])) #loads only the text on the first document
```

### Preview first document

```{r}
#writeLines(as.character(docs[1]))
#writeLines(as.character(docs))
inspect(docs[[1]])
```


### Remove punctuation
```{r}
docs <- tm_map(docs,removePunctuation)
#inspect(docs[[1]])
```


### Identify characters that are NOT letters
```{r}
content <- ""
for(i in 1:length(docs)){
 
  content <- paste(content, paste(as.character(docs[[i]]), collapse = " "))
  
}
notLetters <- unlist(str_extract_all(content, '[^A-Za-z ]'))
notLetters <- unique(unlist(strsplit(notLetters, "[ ]")))
```

### These are the special characters identified
```{r}
kable(notLetters, format="markdown")
```

### Remove the special characters
```{r}
for (j in seq(docs)) {
  for (i in 1:length(notLetters)){
    docs[[j]]$content <- gsub(notLetters[i], " ", docs[[j]])
  }
}
#inspect(docs[[1]])
```

### Remove Whitespace
```{r}
docs <- tm_map(docs, stripWhitespace)
#inspect(docs[[1]])
```

### Remove Numbers
The removal of special characters above also removed numbers
```{r}
docs <- tm_map(docs, removeNumbers) 
#inspect(docs[[1]])
```

### Convert to lowercase
```{r}
docs <- tm_map(docs, tolower)   
docs <- tm_map(docs, PlainTextDocument)
DocsCopy <- docs
#inspect(docs[[1]])
```

### Remove stop words
```{r}
# For a list of the stopwords, see:   
# length(stopwords("english"))   
# stopwords("english")   
docs <- tm_map(docs, removeWords, stopwords("english"))   
docs <- tm_map(docs, PlainTextDocument)
#inspect(docs[[1]])
```

### Removing particular words:
```{r}
docs <- tm_map(docs, removeWords, c("syllogism", "tautology"))   
# Just remove the words "syllogism" and "tautology". 
# These words don't actually exist in these texts. But this is how you would remove them if they had.
```

### Combining words that should stay together

This takes a list of multi-term data science keywords. These keywords will be preserved together by concatenating each term with a symbol to eliminate the whitespace in between the terms. 


#### Preview of list
```{r}
file <-
  "https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/Multi-part%20DS%20terms_Shahparan-Version2.csv"

terms <- 
  read.csv(file, header=TRUE, sep=",", stringsAsFactors = FALSE, row.names = NULL)

kable(head(terms,10), format="markdown")
```

#### Replace the multi-term keywords
```{r}
for (j in seq(docs))
{
  #docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
  for(i in 1:nrow(terms)){
    docs[[j]] <- gsub(terms$Terms[i], terms$Replace[i], docs[[j]])
  }
}
docs <- tm_map(docs, PlainTextDocument)
```


### Remove whitespace
```{r}
docs <- tm_map(docs, stripWhitespace)
inspect(docs[[1]])
```

## Stage the Data

### create a term document matrix.

100 documents with 5,431 terms. 
```{r}
tdm <- TermDocumentMatrix(docs) 
tdm  
```

### Remove Sparse Terms

If sparse = .01, then only terms that appear in (nearly) every document will be retained.

```{r}
#dtms <- removeSparseTerms(dtm, 0.70)
#dim(dtms)
#dtms
tdms <- removeSparseTerms(tdm, 0.99)
tdms
```

## Explore your data

### Preview
```{r}
#m <- as.matrix(tdms)
m <- as.matrix(tdm)

for(i in 1:nrow(m)){
  
  for(j in 1:ncol(m)){
    if(m[i,j]>1){
      m[i,j] <- 1
    }
  }
}

freq <- rowSums(m)
termFreq <- cbind(names(freq),freq)
rownames(termFreq) <- NULL
colnames(termFreq) <- c("term", "frequency")
termFreq <- data.frame(termFreq)

#write.csv(m, file="TermDocumentMatrix_adjusted.csv") 
write.csv(termFreq, file="TermFrequency_adjusted.csv") 
```

### Export to Excel for further investigation
```{r}
m <- as.matrix(tdms)
freq <- rowSums(m)
names(freq)
dim(m)   
#write.csv(m, file="DocumentTermMatrix2.csv")  
write.csv(m, file="TermDocumentMatrix.csv")  
```



