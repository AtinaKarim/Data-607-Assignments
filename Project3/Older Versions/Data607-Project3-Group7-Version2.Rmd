---
title: "Most Valued Data Science Skills"
author: "Group 7"
date: "March 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---------------

### Project 3 

> W. Edwards Deming said, "In God we trust, all others must bring data." Please use data to answer the question, "Which are the most valued data science skills?" Consider your work as an exploration; there is not necessarily a "right answer."

---------------

### Files

* List of 100 job URLs: <br/>
https://github.com/Shetura36/Data-607-Assignments/blob/master/Project3/100Jobs-Shahparan-Version2.csv 

* Job posting text files (scrapped form the web): <br/>
https://github.com/Shetura36/Data-607-Assignments/tree/master/Project3/Jobposting-Archive

* List of term substitution: <br/>
https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/TermsSubstitution_Shahparan-Version5.csv

* List of words removed: <br/>
https://github.com/Shetura36/Data-607-Assignments/blob/master/Project3/RemoveTerms-Score20andHigher-Version3.csv

* Term Frequency output: <br/>
https://github.com/Shetura36/Data-607-Assignments/blob/master/Project3/TermFrequency_adjusted_9.csv


---------------

### Active group members

* AHM Shahparan
* Alejandro Osborne 
* Brian Liles
* Sherranette Tinapunan

---------------

### Load libraries

```{r warning=FALSE, message=FALSE}
library(rvest)
library(knitr)
library(tm)
library(stringr)
library(dplyr)
```

---------------

## Web scraping

### Load job posting URLs

The job URL file contains 100 entries. It has three columns: row_id, job_url, and selector. 
The `selector` column store the selector item as determined by `selectorgadget` tool.

```{r}
#setwd("C:/Users/stina/Documents/GitHub/Data-607-Assignments/Project3")

url_source <- 
  "https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/100Jobs-Shahparan-Version2.csv"

#load the list of URLs
url_listing <- data.frame(read.csv(url_source, header=TRUE, sep=",", stringsAsFactors = FALSE, row.names = NULL))
```

### Preview of URL list
```{r echo=FALSE}
row.names(url_listing) <- NULL
kable(url_listing[12:18,], format="markdown")
```

---------------

### Extract and save the job posting

While manually stepping through each URL to confirm that the selector grabs data from each page, I noticed that some of the URLs started to expire. In addition, there was a post in the class slack channel about being banned from a job site. So while going through the the step by step check, a line of code saved each extracted data to a text file just in case the posting expires or access gets denied. However, when an attempt was made to run the entire for loop to go through the 100 URLs to automatically build the dataset of job postings, Monster.com had already blocked access to the site. 

```{r echo=FALSE, out.width='80%'}
knitr::include_graphics("./images/Monster.com-Forbidden Access-3.23.2018-snippet.png")
```

The code below will go through each URL, scrape the web page for the data specified by the selector, and save the output as a text file. This code will generate 100 text files in the specified directory. The code has been commented out for the reason mentioned above.
```{r}
setwd("./JobpostingText")

#for (i in 1:nrow(url_listing)){
#  read_html(url_listing$job_url[i]) %>% 
#      html_nodes(url_listing$selector[i]) %>% html_text() -> data
  
#  filename <- paste("job", i, ".txt")
#  fileConn<-file(filename)
#  writeLines(c(data), fileConn)
#  close(fileConn)
#}
```


## Text mining and data clean up

The `tm` library is used to clean up the data and build a term document matrix. 
The code below will grab all 100 job postings under the specified directory. 
```{r}
cname <- "./Jobposting-Archive"
docs <- VCorpus(DirSource(cname))   
#summary(docs) 
```

### Preview first document

```{r echo=FALSE}
#writeLines(as.character(docs[1]))
#writeLines(as.character(docs))
inspect(docs[[1]])
```


### Data clean up

The clean up steps below, which is done by a mapper function called `tm_map` of the `tm` library is used to 

* convert to lowercase
* remove punctuation
* remove numbers
* remove unnecessary white spaces
* substitute terms  (list of terms from a file)
* remove stop words
* remove irrelevant words (list of terms from a file)

For the removal of irrelevant words, a pre-processing of the term document matrix was done first so we can investigate the words that are present in the dataset. The term document matrix was adjusted so that if a term is present in a document, it is only counted once. Mentioning the term more than once in a single job posting does not increase its relevance. A term frequency list is generated and reviewed by a team member. Only words with scores of 20 or higher were reviewed to determine if they are irrelevant. Examples of words removed are business, world, high, full, etc.

About preserving the term "r" in the dataset. 

### Convert to lowercase
```{r}
docs <- tm_map(docs, tolower)   
docs <- tm_map(docs, PlainTextDocument)
#inspect(docs[[1]])
```

```{r}
clean.text <- function(text){
  str_replace_all(text, regex('\r\n|\n|\t|\r|,|/|<|>|\\.'), ' ')}
docs <- tm::tm_map(docs, clean.text)
docs <- tm_map(docs, PlainTextDocument)
#inspect(docs[[1]])
```


### Remove punctuation
```{r}
for (j in seq(docs)) {

    docs[[j]]$content <- gsub("[[:punct:][:blank:]]+", " ", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
#inspect(docs[[1]])
```

### Identify characters that are NOT letters
```{r}
content <- ""
for(i in 1:length(docs)){
 
  content <- paste(content, paste(as.character(docs[[i]]), collapse = " "))
  
}
notLetters <- unlist(str_extract_all(content, '[^A-Za-z ]'))
notLetters <- unique(unlist(strsplit(notLetters, " ")))
```

### Characters that are not letters
```{r echo=FALSE}
notLetters
```

### Remove characters that are not letters

Characters that are not letters are going to be replaced by a white space. 
We are not concerned with tracking strings that numbers.

```{r}
for (j in seq(docs)) {
  for (i in 1:length(notLetters)){
    docs[[j]]$content <- gsub(notLetters[i], " ", docs[[j]])
  }
}
docs <- tm_map(docs, stripWhitespace)
#inspect(docs[[1]])
```

```{r echo=FALSE}
#for testing of regular expressions
#docs[[1]] <- gsub("^r[ ]{1,}|[ ]{1,}r[ ]{1,}|[ ]{1,}r$", " r-programming ", docs[[1]], ignore.case = TRUE)
#docs <- tm_map(docs, PlainTextDocument)
#inspect(docs[[1]])
```

### Term substitution

This takes a list of data science terms from a file. This file was prepared by a team member.
This file serves two purpose: (1) to preserve multi-term keywords like "artificial-intelligence", and (2) to group similar terms into a single category. For example terms like decisions, decisions support, decisions tools are substituted with the term decision-science. 

#### Preview list of terms
```{r}
file <-
  "https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/TermsSubstitution_Shahparan-Version5.csv"

terms <- 
  read.csv(file, header=TRUE, sep=",", stringsAsFactors = FALSE, row.names = NULL)

kable(head(terms,10), format="markdown")
#kable(terms)
```


#### Substitute terms from list
```{r}
for (j in seq(docs))
{
  #docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
  for(i in 1:nrow(terms)){
    docs[[j]] <- gsub(terms$Terms[i], terms$Replace[i], docs[[j]], ignore.case = TRUE)
  }
}
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
inspect(docs[[1]])
```


### Remove stop words
```{r}
# For a list of the stopwords, see:   
# length(stopwords("english"))   
#stopwords("english")   
docs <- tm_map(docs, removeWords, stopwords("english"))  
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
#inspect(docs[[1]])
```

### Removing irrelevant words

```{r echo=FALSE}
#file <-
 # "https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/RemoveTerms-Score20andHigher.csv"

#terms <- 
 # read.csv(file, header=TRUE, sep=",", stringsAsFactors = FALSE, row.names = NULL)

#terms <- unlist(terms)

#docs <- tm_map(docs, removeWords, terms)  
#inspect(docs[[1]])
```

The intermediate term frequency output was analyzed by a team member.
These are words that have a frequency of 20 or higher, but are related to skills. 
The code below loads the list of words to remove. 

```{r}
file <-
  "https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/RemoveTerms-Score20andHigher-Version3.csv"

terms <- 
  read.csv(file, header=TRUE, sep=",", stringsAsFactors = FALSE, row.names = NULL)
```

#### Preview list
```{r}
kable(head(terms,10), format="markdown")
#terms$Terms[138]
#terms$Replace[138]
#nrow(terms)
```


#### Remove terms by substituting a white space
```{r}
for (j in seq(docs))
{
  #docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
  for(i in 1:nrow(terms)){
    docs[[j]] <- gsub(terms$Terms[i], terms$Replace[i], docs[[j]], ignore.case = TRUE)
  }
  docs[[j]] <- gsub("xyz", "  ", docs[[j]], ignore.case = TRUE)
}
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
inspect(docs[[1]])
```

-------------------

### Create a term document matrix

There are 100 documents with 5,431 terms.
```{r}
tdm <- TermDocumentMatrix(docs) 
tdm  
```


### Adjust term document matrix count

The code below will update the count to 1 if the count is greater than 1. A term is mentioned at least once in a document, it is only counted once.  Mentioning the term more than once in a single job posting does not increase its relevance.

```{r}
#m <- as.matrix(tdms)
m <- as.matrix(tdm)

#adjust the term count so that when a term appears in a document, it is only counted once
for(i in 1:nrow(m)){
  
  for(j in 1:ncol(m)){
    if(m[i,j]>1){
      m[i,j] <- 1
    }
  }
}
```

### Create the term frequency list

The code below sums the count across all 100 documents for each term. This will assign a frequency score for each term. 
If a term has a score of 50. This means that the term was mentioned on 50 different job postings. 

```{r}
freq <- rowSums(m)
termFreq <- cbind(names(freq),freq)
rownames(termFreq) <- NULL
colnames(termFreq) <- c("term", "frequency")
termFreq <- data.frame(termFreq, stringsAsFactors = FALSE)
termFreq$frequency <- as.numeric(termFreq$frequency)
```

### Terms with frequency scores of 20 and higher
```{r}
result <- 
  termFreq %>% dplyr::select(term, frequency) %>% dplyr::filter(frequency > 19) %>% dplyr::arrange(desc(frequency))
kable(result,format="markdown")
```


### Write the term frequency to a file
```{r}
result <- 
  termFreq %>% dplyr::select(term, frequency) %>% dplyr::arrange(desc(frequency))
#write.csv(m, file="TermDocumentMatrix_adjusted.csv") 
write.csv(result, file="TermFrequency_adjusted_10.csv")
```


